---
title: "Classification"
output: pdf_document
author: "Zackary Frazier"
---

```{r setup, message=FALSE}
library(tidyverse)
library(lubridate)
library(caret)
library(rpart)
library(ROCR)
library(broom)
theme_set(theme_bw())

csv_file <- "Affordability_Wide_2017Q4_Public.csv"
tidy_afford <- read_csv(csv_file) %>%
  filter(Index == "Mortgage Affordability") %>%
  drop_na() %>%
  filter(RegionID != 0, RegionName != "United States") %>%
  dplyr::select(RegionID, RegionName, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))
tidy_afford
```


```{r first_glance}
tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")

```

# Can we predict if mortgage affordability will increase or decrease a year from now?

```{r desired}
outcome_df <- tidy_afford %>%
  mutate(yq = quarter(time, with_year=TRUE)) %>%
  filter(yq %in% c("2016.4", "2017.4")) %>%
  select(RegionID, RegionName, yq, affordability) %>%
  spread(yq, affordability) %>%
  mutate(diff = `2017.4` - `2016.4`) %>%
  mutate(Direction = ifelse(diff>0, "up", "down")) %>%
  select(RegionID, RegionName, Direction)
outcome_df
```

```{r predict}
predictor_df <- tidy_afford %>%
  filter(year(time) <= 2016)
```

Question: Is a decision tree model better than a random forest model for 
this data?

# Date Preparation

Here we combine our predictor with our outcomes. To train our data we'll
need our data to show how affordability changes over time for each region,
so we'll spread the affordability data over the time periods.
```{r prep}
total_df <- predictor_df %>% 
  inner_join(y=outcome_df) %>% 
  spread(time, affordability) %>% 
  select(-RegionName)

# standardize the data
for(i in 3:ncol(total_df)) {
  col_mean <- sapply(total_df[,i], mean)
  col_sd <- sapply(total_df[,i], sd)
  for(k in 1:nrow(total_df)) {
    total_df[k, i] <- (total_df[k, i] - col_mean) / col_sd
  }
}

head(total_df)

```


Now we create the 10-folds and create the training set and the testing set.

```{r models}
set.seed(1234)

partitionRule <- createFolds(total_df$Direction, k=10, list=F)
trainingSet <- total_df[partitionRule,]
testingSet <- total_df[-partitionRule,]

names(trainingSet) <- make.names(colnames(trainingSet))
names(testingSet) <- make.names(colnames(testingSet))

splitRule <- trainControl(method='cv', 
                          number=10,
                          classProbs=TRUE,
                          summaryFunction=twoClassSummary)
```



Here I test the accuracy of our models using predictions and express the results as a confusion matrix. 
I use the predict function instead of the train function because the train function keeps causing problems 
when I run the prediction function.

# Decision Tree
```{r dec_tree}
tree <- rpart(Direction~., data=trainingSet)
treePred <- predict(tree, newdata=testingSet, type='vector')
tp <- prediction(treePred, testingSet$Direction)

treePred[treePred == 1] <- 'down'
treePred[treePred == 2] <- 'up'

confusionMatrix(factor(treePred), factor(testingSet$Direction))
```

Here I rest the predictions made by a random forest. Interestingly, it's predictions are less accurate than
the decision tree. This may imply that the vastness of the amounts of data are skewing the overall effeciveness.

# Random Forests
```{r forests, warning=F}
library(randomForest)
forest <- randomForest(ifelse(Direction == 'up', 1, 0)~.,data=trainingSet, type='raw')
forestPred <- predict(forest, newdata=testingSet, type='response')
fp <- prediction(forestPred, testingSet$Direction)

forestPred[forestPred >= 0.5] <- 'up'
forestPred[forestPred < 0.5] <- 'down'

confusionMatrix(factor(forestPred), factor(testingSet$Direction))
```

Here I generate three functions for getting ROC data each taylored to the different models.

```{r ROC}

# a function to obtain performance data
# (tpr and fpr) over the given cross validation
get_roc_data_tree <- function(df, ntree, cv_partition, type, fit_control) {
  mean_fpr <- seq(0, 1, len=100)
  aucs <- numeric(length(cv_partition))
  
  res <- lapply(seq_along(cv_partition),  function(i) {
    fit <- rpart(Direction~., data=trainingSet)
    
    preds <- predict(fit, newdata=testingSet,type="vector")
    
    perf <- ROCR::prediction(preds, testingSet$Direction) %>%
      ROCR::performance(measure="tpr", x.measure="fpr")

    fpr <- unlist(perf@x.values)
    tpr <- unlist(perf@y.values)
    

    interp_tpr <- approxfun(fpr, tpr)(mean_fpr)
    interp_tpr[1] <- 0.0
  
    
    data_frame(fold=rep(i, length(mean_fpr)), fpr=mean_fpr, tpr=interp_tpr)
  })
  

  do.call(rbind, res)
}

get_roc_data_forest <- function(df, ntree, cv_partition, type, fit_control) {
  mean_fpr <- seq(0, 1, len=100)
  aucs <- numeric(length(cv_partition))
  test <- testingSet 
  test$Direction[test$Direction == 'up'] <- 1
  test$Direction[test$Direction == 'down'] <- 0
  
  res <- lapply(seq_along(cv_partition),  function(i) {
    fit <- randomForest(ifelse(Direction == 'up', 1, 0)~.,
                        data=trainingSet, type='raw')
    
    preds <- predict(fit, newdata=testingSet,type="response")
    preds[preds >= 0.5] <- 1
    preds[forestPred < 0.5] <- 0
    
    perf <- ROCR::prediction(preds, test$Direction) %>%
      ROCR::performance(measure="tpr", x.measure="fpr")

    fpr <- unlist(perf@x.values)
    tpr <- unlist(perf@y.values)
    

    interp_tpr <- approxfun(fpr, tpr)(mean_fpr)
    interp_tpr[1] <- 0.0
  
    
    data_frame(fold=rep(i, length(mean_fpr)), fpr=mean_fpr, tpr=interp_tpr)
  })
  

  do.call(rbind, res)
}


compute_auc <- function(curve_df) {
  curve_df %>% 
    group_by(fold) %>%
    summarize(auc=pracma::trapz(fpr, tpr))
}

```

Here I get the performance data for 500 trees and 500 random forests. This allows me to retrieve a sufficiently
large enough amount of data for ROC and AUROC analysis.

```{r trees, warning=F}

curve_tree <- get_roc_data_tree(df=total_df, ntree=500, cv_partition=partitionRule,
                                      type='rpart', fit_control=splitRule) %>% 
  mutate(model="tree")
auc_tree <- compute_auc(curve_tree) %>% 
  mutate(model="tree")


curve_forest <- get_roc_data_forest(df=total_df, ntree=500,
                        cv_partition=partitionRule, 
                        type='forest', fit_control=splitRule) %>% 
  mutate(model="forest")
auc_forest <- compute_auc(curve_forest) %>% 
  mutate(model="forest")
```

Here I compare the AUC of each the tree model and the forest model. These analyses are expressed as two 
different graphs because the combination of the data into one graph causes the forest data to appear as a
small dot, and that's not very helpful for analysis. It appears the tree model has significantly more variance
as the threshold changes while the forest model's AUC consistently has a value of 0.6.

```{r auc}

ggplot(auc_tree, aes(x=model, y=auc)) +
  geom_jitter(position=position_jitter(0.1)) +
  coord_flip() + 
  labs(title="AUC comparision Tree",
       x="Model",
       y="Area under ROC curve")
ggplot(auc_forest, aes(x=model, y=auc)) +
  geom_jitter(position=position_jitter(0.1)) +
  coord_flip() + 
  labs(title="AUC comparision Forest",
       x="Model",
       y="Area under ROC curve")
```

Now we use linear regression to analyze the differences between the models. The estimate being positive
shows that the tree model is slightly better at predicting the the results. The small p.value indicates
that we can ignore the null hypothesis that both models are equivalent in terms of measuring the data.
``` {r values}
library(broom)

lm(auc~model, data=rbind(auc_forest, auc_tree)) %>%
  tidy() 

```

Here we can see a side-by-side comparison of the ROC curves of the tree model and the random forest model.
Clearly we can see from a visual analysis that the area under the tree's curve is greater than the area 
under the random forest's curve.

```{r test_for_diff}

curve_tree  %>%
  group_by(model, fpr) %>%
  summarize(tpr = mean(tpr)) %>%
  ggplot(aes(x=fpr, y=tpr, color=model)) +
    geom_line() +
    labs(title = "Tree ROC curve",
         x = "False positive rate",
         y = "True positive rate")

curve_forest%>%
  group_by(model, fpr) %>%
  summarize(tpr = mean(tpr)) %>%
  ggplot(aes(x=fpr, y=tpr, color=model)) +
    geom_line() +
    labs(title = "Random Forest ROC curve",
         x = "False positive rate",
         y = "True positive rate")

``` 


