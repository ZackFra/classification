---
title: "Classification"
output: pdf_document
---

```{r setup}
library(tidyverse)
library(lubridate)
library(caret)
library(rpart)
theme_set(theme_bw())

csv_file <- "Affordability_Wide_2017Q4_Public.csv"
tidy_afford <- read_csv(csv_file) %>%
  filter(Index == "Mortgage Affordability") %>%
  drop_na() %>%
  filter(RegionID != 0, RegionName != "United States") %>%
  dplyr::select(RegionID, RegionName, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))
tidy_afford
```


```{r first_glance}
tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")

```

# Can we predict if mortgage affordability will increase or decrease a year from now?

```{r desired}
outcome_df <- tidy_afford %>%
  mutate(yq = quarter(time, with_year=TRUE)) %>%
  filter(yq %in% c("2016.4", "2017.4")) %>%
  select(RegionID, RegionName, yq, affordability) %>%
  spread(yq, affordability) %>%
  mutate(diff = `2017.4` - `2016.4`) %>%
  mutate(Direction = ifelse(diff>0, "up", "down")) %>%
  select(RegionID, RegionName, Direction)
outcome_df
```

```{r predict}
predictor_df <- tidy_afford %>%
  filter(year(time) <= 2016)
```

Question: Is a decision tree model better than a random forest model for 
this data?

# Date Preparation

Here we combine our predictor with our outcomes. To train our data we'll
need our data to show how affordability changes over time for each region,
so we'll spread the affordability data over the time periods.
```{r prep}
total_df <- predictor_df %>% 
  inner_join(y=outcome_df) %>% 
  spread(time, affordability) %>% 
  select(-RegionName) 
total_df


```


Now we divide our set into our training set and our testing set

```{r models, message=FALSE}
set.seed(1234)

partitionRule <- createFolds(total_df$Direction, k=10, list=F)
trainingSet <- total_df[partitionRule,]
testingSet <- total_df[-partitionRule,]

names(trainingSet) <- make.names(colnames(trainingSet))
names(testingSet) <- make.names(colnames(testingSet))

splitRule <- trainControl(method='cv', 
                          number=10,
                          classProbs=TRUE,
                          summaryFunction=twoClassSummary)

cforestModel <- train(Direction~.,
                  data=trainingSet,
                  trControl=splitRule,
                  method='cforest',
                  preProc = c("center", "scale"),
                  metric='ROC')

treeModel <- train(Direction~., 
                  data=trainingSet,
                  trControl=splitRule,
                  method='rpart',
                  preProc = c("center", "scale"),
                  metric='ROC')
```



testing the accuracy of our models

# Decision Tree
```{r tree}
tree <- rpart(Direction~., data=trainingSet)
treePred <- predict(tree, newdata=testingSet, type='vector')

tp <- prediction(treePred, testingSet$Direction)
```

# Random Forests
```{r forests}
library(randomForest)
forestPred <- predict(cforestModel, newdata=testingSet, type='raw')
length(forestPred)
length(testingSet)
prediction(forestPred, testingSet$Direction)
length(forestPred)
forestTab
```

```{r ROC}

# a function to obtain performance data
# (tpr and fpr) over the given cross validation
get_roc_data <- function(df, ntree, cv_partition, type, fit_control) {
  mean_fpr <- seq(0, 1, len=100)
  aucs <- numeric(length(cv_partition))
  
  res <- lapply(seq_along(cv_partition),  function(i) {
    fit <- rpart(Direction~., data=trainingSet)
    
    preds <- predict(fit, newdata=testingSet,type="vector")
    

    perf <- ROCR::prediction(preds, testingSet$Direction) %>%
      ROCR::performance(measure="tpr", x.measure="fpr")

    fpr <- unlist(perf@x.values)
    tpr <- unlist(perf@y.values)
    

    interp_tpr <- approxfun(fpr, tpr)(mean_fpr)
    interp_tpr[1] <- 0.0
  
    
    data_frame(fold=rep(i, length(mean_fpr)), fpr=mean_fpr, tpr=interp_tpr)
  })
  

  do.call(rbind, res)
}

compute_auc <- function(curve_df) {
  curve_df %>% 
    group_by(fold) %>%
    summarize(auc=pracma::trapz(fpr, tpr))
}
```

Get the performance data for 10 trees.

```{r trees}

small_curve_df <- get_roc_data(df=total_df, ntree = 10, 
                               cv_partition=partitionRule, type="rpart", fit_control=splitRule)
small_auc_df <- compute_auc(small_curve_df)

large_curve_df <- get_roc_data(df=total_df, ntree=500, cv_partition=partitionRule, type='rpart',
                               fit_control=splitRule)
large_auc_df <- compute_auc(large_curve_df)
```

```{r graph}
curve_df <- small_curve_df %>%
  mutate(model="small") %>%
  rbind(mutate(large_curve_df, model="large")) %>%
  mutate(model = factor(model, levels=c("small", "large")))

auc_df <- small_auc_df %>%
  mutate(model="small") %>%
  rbind(mutate(large_auc_df, model="large")) %>%
  mutate(model = factor(model, levels=c("small", "large")))

curve_df <- small_curve_df %>%
  mutate(model="small") %>%
  rbind(mutate(large_curve_df, model="large")) %>%
  mutate(model = factor(model, levels=c("small", "large")))

auc_df <- small_auc_df %>%
  mutate(model="small") %>%
  rbind(mutate(large_auc_df, model="large")) %>%
  mutate(model = factor(model, levels=c("small", "large")))

# plot distribution of 
ggplot(auc_df, aes(x=model, y=auc)) +
  geom_jitter(position=position_jitter(0.1)) +
  coord_flip() + 
  labs(title="AUC comparision",
       x="Model",
       y="Area under ROC curve")
```


```{r test_for_diff}
library(broom)

model_tab <- auc_df %>%
  lm(auc~model,data=.) %>%
  tidy() 

model_tab %>%
  knitr::kable()

curve_df %>%
  group_by(model, fpr) %>%
  summarize(tpr = mean(tpr)) %>%
  ggplot(aes(x=fpr, y=tpr, color=model)) +
    geom_line() +
    labs(title = "ROC curves",
         x = "False positive rate",
         y = "True positive rate")

```