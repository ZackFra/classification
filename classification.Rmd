---
title: "Classification"
output: pdf_document
---

```{r setup}
library(tidyverse)
library(lubridate)
library(caret)
library(rpart)
theme_set(theme_bw())

csv_file <- "Affordability_Wide_2017Q4_Public.csv"
tidy_afford <- read_csv(csv_file) %>%
  filter(Index == "Mortgage Affordability") %>%
  drop_na() %>%
  filter(RegionID != 0, RegionName != "United States") %>%
  dplyr::select(RegionID, RegionName, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))
tidy_afford
```


```{r first_glance}
tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")

```

# Can we predict if mortgage affordability will increase or decrease a year from now?

```{r desired}
outcome_df <- tidy_afford %>%
  mutate(yq = quarter(time, with_year=TRUE)) %>%
  filter(yq %in% c("2016.4", "2017.4")) %>%
  select(RegionID, RegionName, yq, affordability) %>%
  spread(yq, affordability) %>%
  mutate(diff = `2017.4` - `2016.4`) %>%
  mutate(Direction = ifelse(diff>0, "up", "down")) %>%
  select(RegionID, RegionName, Direction)
outcome_df
```

```{r predict}
predictor_df <- tidy_afford %>%
  filter(year(time) <= 2016)
```

Question: Is a decision tree model better than a random forest model for 
this data?

# Date Preparation

Here we combine our predictor with our outcomes. To train our data we'll
need our data to show how affordability changes over time for each region,
so we'll spread the affordability data over the time periods.
```{r prep}
total_df <- predictor_df %>% 
  inner_join(y=outcome_df) %>% 
  spread(time, affordability) %>% 
  select(-RegionName)
total_df$Direction <- as.factor(total_df$Direction)
total_df
```


Now we divide our set into our training set and our testing set

```{r models, message=FALSE}
set.seed(1234)

partitionRule <- createFolds(total_df$Direction, k=10, list=F)
trainingSet <- total_df[partitionRule,]
testingSet <- total_df[-partitionRule,]

names(trainingSet) <- make.names(colnames(trainingSet))
names(testingSet) <- make.names(colnames(testingSet))

splitRule <- trainControl(method='cv', 
                          number=10,
                          classProbs=TRUE,
                          summaryFunction=twoClassSummary)

treeModel <- train(Direction~., 
                  data=trainingSet,
                  trControl=splitRule,
                  method='rpart',
                  metric='ROC')

cforestModel <- train(Direction~.,
                  data=trainingSet,
                  trControl=splitRule,
                  method='cforest',
                  metric='ROC')
                          
```


testing the accuracy of our models

# Decision Tree
```{r tree}
treePred <- predict(treeModel, newdata=testingSet)
confusionMatrix(data=treePred, testingSet$Direction)
```

# Random Forests
```{r forests}
cforestPred <- predict(cforestModel, newdata=testingSet)
confusionMatrix(data=cforestPred, testingSet$Direction)
```

```{r ROC}
library(pROC)
library(tree)
tree.fit <- tree(Direction~., data=testingSet)

roc(Direction,, plot=T)
```
