---
title: "Classification"
output: pdf_document
---

```{r setup}
library(tidyverse)
library(lubridate)
library(caret)
library(rpart)
library(ROCR)
library(broom)
theme_set(theme_bw())

csv_file <- "Affordability_Wide_2017Q4_Public.csv"
tidy_afford <- read_csv(csv_file) %>%
  filter(Index == "Mortgage Affordability") %>%
  drop_na() %>%
  filter(RegionID != 0, RegionName != "United States") %>%
  dplyr::select(RegionID, RegionName, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))
tidy_afford
```


```{r first_glance}
tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")

```

# Can we predict if mortgage affordability will increase or decrease a year from now?

```{r desired}
outcome_df <- tidy_afford %>%
  mutate(yq = quarter(time, with_year=TRUE)) %>%
  filter(yq %in% c("2016.4", "2017.4")) %>%
  select(RegionID, RegionName, yq, affordability) %>%
  spread(yq, affordability) %>%
  mutate(diff = `2017.4` - `2016.4`) %>%
  mutate(Direction = ifelse(diff>0, "up", "down")) %>%
  select(RegionID, RegionName, Direction)
outcome_df
```

```{r predict}
predictor_df <- tidy_afford %>%
  filter(year(time) <= 2016)
```

Question: Is a decision tree model better than a random forest model for 
this data?

# Date Preparation

Here we combine our predictor with our outcomes. To train our data we'll
need our data to show how affordability changes over time for each region,
so we'll spread the affordability data over the time periods.
```{r prep}
total_df <- predictor_df %>% 
  inner_join(y=outcome_df) %>% 
  spread(time, affordability) %>% 
  select(-RegionName)
total_df


for(i in 3:ncol(total_df)) {
  col_mean <- sapply(total_df[,i], mean)
  col_sd <- sapply(total_df[,i], sd)
  for(k in 1:nrow(total_df)) {
    total_df[k, i] <- (total_df[k, i] - col_mean) / col_sd
  }
}

total_df

```


Now we divide our set into our training set and our testing set

```{r models}
set.seed(1234)

partitionRule <- createFolds(total_df$Direction, k=10, list=F)
trainingSet <- total_df[partitionRule,]
testingSet <- total_df[-partitionRule,]

names(trainingSet) <- make.names(colnames(trainingSet))
names(testingSet) <- make.names(colnames(testingSet))

splitRule <- trainControl(method='cv', 
                          number=10,
                          classProbs=TRUE,
                          summaryFunction=twoClassSummary)
```



Testing the accuracy of our models. I use the predict function instead of the train function because
the train function keeps causing problems when run with prediction.

# Decision Tree
```{r tree}
tree <- rpart(Direction~., data=trainingSet)
treePred <- predict(tree, newdata=testingSet, type='vector')
tp <- prediction(treePred, testingSet$Direction)

treePred[treePred == 1] <- 'down'
treePred[treePred == 2] <- 'up'

confusionMatrix(factor(treePred), factor(testingSet$Direction))
```

# Random Forests
```{r forests}
library(randomForest)
forest <- randomForest(ifelse(Direction == 'up', 1, 0)~.,data=trainingSet, type='raw')
forestPred <- predict(forest, newdata=testingSet, type='response')
fp <- prediction(forestPred, testingSet$Direction)

forestPred[forestPred >= 0.5] <- 'up'
forestPred[forestPred < 0.5] <- 'down'

confusionMatrix(factor(forestPred), factor(testingSet$Direction))
```

```{r ROC}

# a function to obtain performance data
# (tpr and fpr) over the given cross validation
get_roc_data_tree <- function(df, ntree, cv_partition, type, fit_control) {
  mean_fpr <- seq(0, 1, len=100)
  aucs <- numeric(length(cv_partition))
  
  res <- lapply(seq_along(cv_partition),  function(i) {
    fit <- rpart(Direction~., data=trainingSet)
    
    preds <- predict(fit, newdata=testingSet,type="vector")
    
    perf <- ROCR::prediction(preds, testingSet$Direction) %>%
      ROCR::performance(measure="tpr", x.measure="fpr")

    fpr <- unlist(perf@x.values)
    tpr <- unlist(perf@y.values)
    

    interp_tpr <- approxfun(fpr, tpr)(mean_fpr)
    interp_tpr[1] <- 0.0
  
    
    data_frame(fold=rep(i, length(mean_fpr)), fpr=mean_fpr, tpr=interp_tpr)
  })
  

  do.call(rbind, res)
}

get_roc_data_forest <- function(df, ntree, cv_partition, type, fit_control) {
  mean_fpr <- seq(0, 1, len=100)
  aucs <- numeric(length(cv_partition))
  test <- testingSet 
  test$Direction[test$Direction == 'up'] <- 1
  test$Direction[test$Direction == 'down'] <- 0
  
  res <- lapply(seq_along(cv_partition),  function(i) {
    fit <- randomForest(ifelse(Direction == 'up', 1, 0)~.,
                        data=trainingSet, type='raw')
    
    preds <- predict(fit, newdata=testingSet,type="response")
    preds[preds >= 0.5] <- 1
    preds[forestPred < 0.5] <- 0
    
    perf <- ROCR::prediction(preds, test$Direction) %>%
      ROCR::performance(measure="tpr", x.measure="fpr")

    fpr <- unlist(perf@x.values)
    tpr <- unlist(perf@y.values)
    

    interp_tpr <- approxfun(fpr, tpr)(mean_fpr)
    interp_tpr[1] <- 0.0
  
    
    data_frame(fold=rep(i, length(mean_fpr)), fpr=mean_fpr, tpr=interp_tpr)
  })
  

  do.call(rbind, res)
}


compute_auc <- function(curve_df) {
  curve_df %>% 
    group_by(fold) %>%
    summarize(auc=pracma::trapz(fpr, tpr))
}

```

Get the performance data for 500 trees.

```{r trees}

curve_tree <- get_roc_data_tree(df=total_df, ntree=500, cv_partition=partitionRule,
                                      type='rpart', fit_control=splitRule) %>% 
  mutate(model="tree")
auc_tree <- compute_auc(curve_tree) %>% 
  mutate(model="tree")


curve_forest <- get_roc_data_forest(df=total_df, ntree=500, cv_partition=partitionRule, 
                                    type='forest', fit_control=splitRule) %>% 
  mutate(model="forest")
auc_forest <- compute_auc(curve_forest) %>% 
  mutate(model="forest")
```

```{r tree}
# plot distribution of 
ggplot(auc_tree, aes(x=model, y=auc)) +
  geom_jitter(position=position_jitter(0.1)) +
  coord_flip() + 
  labs(title="AUC comparision Tree",
       x="Model",
       y="Area under ROC curve")
ggplot(auc_forest, aes(x=model, y=auc)) +
  geom_jitter(position=position_jitter(0.1)) +
  coord_flip() + 
  labs(title="AUC comparision Forest",
       x="Model",
       y="Area under ROC curve")
```


```{r test_for_diff_tree}

tree_ROC_plot <- curve_tree  %>%
  group_by(model, fpr) %>%
  summarize(tpr = mean(tpr)) %>%
  ggplot(aes(x=fpr, y=tpr, color=model)) +
    geom_line() +
    labs(title = "ROC curves",
         x = "False positive rate",
         y = "True positive rate")

forest_ROC_plot <- curve_forest%>%
  group_by(model, fpr) %>%
  summarize(tpr = mean(tpr)) %>%
  ggplot(aes(x=fpr, y=tpr, color=model)) +
    geom_line() +
    labs(title = "ROC curves",
         x = "False positive rate",
         y = "True positive rate")

ggarrange(tree_ROC_plot, forest_ROC_plot)
``` 

```{r test_for_diff_forest}


```

